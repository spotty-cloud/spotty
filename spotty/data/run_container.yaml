Description: Spotty EC2 Spot Instance
Parameters:
  VpcId:
    Description: VPC ID
    Type: AWS::EC2::VPC::Id
  InstanceProfileArn:
    Description: Instance Profile ARN
    Type: String
  InstanceType:
    Description: Instance type to launch EC2 Spot Instance
    Type: String
  KeyName:
    Description: Key name to get an access o the instance
    Type: AWS::EC2::KeyPair::KeyName
  ImageId:
    Description: AMI ID
    Type: AWS::EC2::Image::Id
  RootVolumeSize:
    Description: Root volume size
    Type: String
  VolumeMountDirectories:
    Description: Directories where the volumes should be mounted (bash array)
    Type: String
    Default: ''
  DockerDataRootDirectory:
    Description: Docker data root directory
    Type: String
    Default: ''
  DockerImage:
    Description: Docker image to run
    Type: String
    Default: ''
  DockerfilePath:
    Description: Dockerfile to build and to use instead of the image
    Type: String
    Default: ''
  DockerNvidiaRuntime:
    Description: Run Docker container with NVIDIA runtime
    Type: String
    Default: 'true'
    AllowedValues: ['true', 'false']
  DockerWorkingDirectory:
    Description: Working directory inside Docker container
    Type: String
    Default: ''
  ProjectS3Bucket:
    Description: S3 bucket with the project
    Type: String
    Default: ''
  ProjectDirectory:
    Description: Destination directory for the project
    Type: String
    Default: ''
Resources:
  SpotInstance:
    Type: AWS::EC2::Instance
    Properties:
      LaunchTemplate:
        LaunchTemplateId: !Ref SpotInstanceLaunchTemplate
        Version: !GetAtt SpotInstanceLaunchTemplate.LatestVersionNumber

  SpotInstanceLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateData:
        InstanceType: !Ref InstanceType
        ImageId: !Ref ImageId
        KeyName: !Ref KeyName
        EbsOptimized: 'false'
        IamInstanceProfile:
          Arn: !Ref InstanceProfileArn
        SecurityGroupIds:
        - !Ref InstanceSecurityGroup
        InstanceInitiatedShutdownBehavior: terminate
        InstanceMarketOptions:
          MarketType: spot
          SpotOptions:
            SpotInstanceType: one-time
            InstanceInterruptionBehavior: terminate
        BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
            VolumeSize: !Ref RootVolumeSize
            DeleteOnTermination: true
        UserData:
          'Fn::Base64': !Sub |
            #!/bin/bash -x

            # install CloudFormation tools
            apt-get update
            apt-get install -y python-setuptools
            mkdir -p aws-cfn-bootstrap-latest
            curl https://s3.amazonaws.com/cloudformation-examples/aws-cfn-bootstrap-latest.tar.gz | tar xz -C aws-cfn-bootstrap-latest --strip-components 1
            easy_install aws-cfn-bootstrap-latest

            # prepare the instance and run Docker container
            /usr/local/bin/cfn-init \
              --stack ${AWS::StackName} \
              --resource SpotInstanceLaunchTemplate \
              --region ${AWS::Region} \
              -c init \
              -v

            # send signal that the Docker container is ready or failed
            /usr/local/bin/cfn-signal \
              -e $? \
              --stack ${AWS::StackName} \
              --region ${AWS::Region} \
              --resource DockerReadyWaitCondition
    Metadata:
      'AWS::CloudFormation::Init':
        configSets:
          init:
            - cwlogs_config
            - prepare_instance_config
            - mount_volumes_config
            - sync_project_config
            - docker_container_config
        cwlogs_config:
          files:
            /tmp/cwlogs/logs.conf:
              content: !Sub |
                [general]
                state_file= /var/awslogs/agent-state

                [/var/log/cloud-init-output.log]
                file = /var/log/cloud-init-output.log
                log_group_name = ${SpotInstanceLogGroup}
                log_stream_name = {instance_id}/cloud-init-output.log
                datetime_format = %d/%b/%Y:%H:%M:%S

                [/var/log/cfn-init.log]
                file = /var/log/cfn-init.log
                log_group_name = ${SpotInstanceLogGroup}
                log_stream_name = {instance_id}/cfn-init.log
                datetime_format = %d/%b/%Y:%H:%M:%S

                [/var/log/cfn-init-cmd.log]
                file = /var/log/cfn-init-cmd.log
                log_group_name = ${SpotInstanceLogGroup}
                log_stream_name = {instance_id}/cfn-init-cmd.log
                datetime_format = %d/%b/%Y:%H:%M:%S

                [/var/log/cfn-hup.log]
                file = /var/log/cfn-hup.log
                log_group_name = ${SpotInstanceLogGroup}
                log_stream_name = {instance_id}/cfn-hup.log
                datetime_format = %d/%b/%Y:%H:%M:%S

                [/var/log/cfn-wire.log]
                file = /var/log/cfn-wire.log
                log_group_name = ${SpotInstanceLogGroup}
                log_stream_name = {instance_id}/cfn-wire.log
                datetime_format = %d/%b/%Y:%H:%M:%S

                [/var/log/docker-commands.log]
                file = /var/log/docker-commands.log
                log_group_name = ${SpotInstanceLogGroup}
                log_stream_name = {instance_id}/docker-commands.log
                datetime_format = %d/%b/%Y:%H:%M:%S
              mode: '000400'
              owner: root
              group: root
            /tmp/cwlogs/run.sh:
              content: !Sub |
                curl https://s3.amazonaws.com//aws-cloudwatch/downloads/latest/awslogs-agent-setup.py -O
                chmod +x ./awslogs-agent-setup.py
                ./awslogs-agent-setup.py -n -r ${AWS::Region} -c /tmp/cwlogs/logs.conf
              mode: '000400'
              owner: root
              group: root
          commands:
            run_cw_agent:
              command: '/bin/bash -xe /tmp/cwlogs/run.sh'
        prepare_instance_config:
          files:
            /tmp/scripts/prepare_instance.sh:
              owner: root
              group: root
              mode: '000400'
              content: !Sub |
                # install AWS CLI
                apt-get install -y awscli
                aws configure set default.region ${AWS::Region}

                # install jq
                apt-get install -y jq

                # create directory for "spotty run" logs
                mkdir -p /var/log/spotty-run
                chmod 777 /var/log/spotty-run
          commands:
            prepare_instance:
              command: '/bin/bash -xe /tmp/scripts/prepare_instance.sh'
        mount_volumes_config:
          files:
            /tmp/scripts/mount_volumes.sh:
              owner: root
              group: root
              mode: '000400'
              content: !Sub |
                DEVICE_LETTERS=(f g h i j k l m n o p)
                MOUNT_DIRS=(${VolumeMountDirectories})

                for i in ${!!MOUNT_DIRS[*]}
                do
                  DEVICE=/dev/xvd${!DEVICE_LETTERS[$i]}
                  MOUNT_DIR=${!MOUNT_DIRS[$i]}

                  blkid -o value -s TYPE $DEVICE || mkfs -t ext4 $DEVICE
                  mkdir -p $MOUNT_DIR
                  mount $DEVICE $MOUNT_DIR
                  chown -R ubuntu:ubuntu $MOUNT_DIR
                done
          commands:
            mount_volumes:
              command: '/bin/bash -xe /tmp/scripts/mount_volumes.sh'
        sync_project_config:
          files:
            /tmp/scripts/sync_project.sh:
              owner: root
              group: root
              mode: '000400'
              content: !Sub |
                if [ -n "${ProjectS3Bucket}" ] && [ -n "${ProjectDirectory}" ]; then
                  mkdir -p ${ProjectDirectory}
                  aws s3 sync s3://${ProjectS3Bucket}/project ${ProjectDirectory}
                fi
          commands:
            sync_project:
              command: '/bin/bash -xe /tmp/scripts/sync_project.sh'
        docker_container_config:
          files:
            /tmp/scripts/run_container.sh:
              owner: root
              group: root
              mode: '000400'
              content: !Sub |
                # change docker data root directory
                if [ -n "${DockerDataRootDirectory}" ]; then
                  jq '. + { "data-root": "${DockerDataRootDirectory}" }' /etc/docker/daemon.json > /tmp/docker_daemon.json \
                    && mv /tmp/docker_daemon.json /etc/docker/daemon.json
                  service docker restart

                  # remove all containers
                  docker rm $(docker ps -aq)
                fi

                # build docker image
                DOCKER_IMAGE="${DockerImage}"
                if [ -n "${DockerfilePath}" ] && [ -n "${ProjectDirectory}" ]; then
                  DOCKER_IMAGE=spotty:`date +%s`
                  docker build \
                    -t $DOCKER_IMAGE \
                    -f ${ProjectDirectory}/${DockerfilePath} \
                    ${ProjectDirectory}
                fi

                if [ -n "$DOCKER_IMAGE" ]; then
                  MOUNT_DIRS=(${VolumeMountDirectories})
                  VOLUME_PARAMS=""
                  for mount_dir in ${!MOUNT_DIRS[*]}
                  do
                    VOLUME_PARAMS="$VOLUME_PARAMS -v $mount_dir:$mount_dir"
                  done

                  NVIDIA_RUNTIME=""
                  if [ "${DockerNvidiaRuntime}" == "true" ]; then
                    NVIDIA_RUNTIME="--runtime=nvidia"
                  fi

                  # create directory for Docker scripts
                  mkdir -p /tmp/docker
                  chmod 777 /tmp/docker

                  # run docker container
                  docker run $NVIDIA_RUNTIME --net=host -td $VOLUME_PARAMS \
                    -v /root/.aws:/root/.aws -v /tmp/docker:/tmp/docker -v ${ProjectDirectory}:${ProjectDirectory} \
                    --name spotty $DOCKER_IMAGE /bin/sh

                  WORKING_DIR=""
                  if [ -n "${DockerWorkingDirectory}" ]; then
                    WORKING_DIR="-w ${DockerWorkingDirectory}"

                    # create working directory if it doesn't exist
                    docker exec spotty mkdir -p ${DockerWorkingDirectory}
                  fi

                  # run initial user commands
                  docker exec $WORKING_DIR spotty /bin/bash -xe /tmp/docker/docker_commands.sh \
                    > /var/log/docker-commands.log 2>&1
                fi
            /tmp/docker/docker_commands.sh:
              owner: root
              group: root
              mode: '000400'
              content: 'echo "Nothing to do"'
          commands:
            run_container:
              command: /bin/bash -xe /tmp/scripts/run_container.sh

  SpotInstanceLogGroup:
    Type: AWS::Logs::LogGroup
    DeletionPolicy: Retain
    Properties:
      RetentionInDays: 1

  InstanceSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      VpcId: !Ref VpcId
      GroupDescription: Spotty security group
      SecurityGroupEgress:
      - CidrIp: 0.0.0.0/0
        IpProtocol: -1
        FromPort: 0
        ToPort: 65535
      - CidrIpv6: ::/0
        IpProtocol: -1
        FromPort: 0
        ToPort: 65535
      SecurityGroupIngress:
      - CidrIp: 0.0.0.0/0
        IpProtocol: tcp
        FromPort: 22
        ToPort: 22
      - CidrIpv6: ::/0
        IpProtocol: tcp
        FromPort: 22
        ToPort: 22

  # function to terminate the instance before detaching the volume
  # (otherwise the volume will be in busy state, because it's mounted to the instance)
  TerminateInstance:
    Type: Custom::InstanceTermination
    DependsOn: []
    Properties:
      ServiceToken: !GetAtt TerminateInstanceFunction.Arn
      InstanceId: !Ref SpotInstance
  TerminateInstanceFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt TerminateInstanceLambdaExecutionRole.Arn
      Runtime: nodejs4.3
      Timeout: 30
      Code:
        ZipFile: !Sub |
          var aws=require("aws-sdk");var response=require('cfn-response');exports.handler=function(event,context){console.log("request received:\n"+JSON.stringify(event));var physicalId=event.PhysicalResourceId;function success(data){data=data||{}
          console.log('SUCCESS:\n',data);return response.send(event,context,response.SUCCESS,data,physicalId)}
          function failed(err){console.log('FAILED:\n',err);return response.send(event,context,response.FAILED,err,physicalId)}
          if(event.RequestType!=='Delete'){console.log('Non-delete request is ignored');return success()}
          var instanceId=event.ResourceProperties.InstanceId;if(!instanceId){return failed('InstanceId required')}
          var ec2=new aws.EC2({region:event.ResourceProperties.Region});ec2.terminateInstances({InstanceIds:[instanceId]}).promise().then((data)=>{console.log('"terminateInstances" Response:\n',JSON.stringify(data));success()}).catch((err)=>failed(err))}
  TerminateInstanceLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
              - lambda.amazonaws.com
          Action:
            - 'sts:AssumeRole'
      Path: /
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      - arn:aws:iam::aws:policy/service-role/AWSLambdaRole
      Policies:
      - PolicyName: EC2Policy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
            - Effect: Allow
              Action:
              - 'ec2:TerminateInstances'
              Resource: ['*']
  TerminateInstanceFunctionRetention:
    Type: Custom::LogsRetention
    DependsOn: TerminateInstance
    Properties:
      ServiceToken: !GetAtt SetLogsRetentionFunction.Arn
      LogGroupName: !Join ['', ['/aws/lambda/', !Ref TerminateInstanceFunction]]
      RetentionInDays: 1

  # function to delete the snapshot that was used to create the volume
  DeleteSnapshotFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt DeleteSnapshotLambdaExecutionRole.Arn
      Runtime: nodejs4.3
      Timeout: 30
      Code:
        ZipFile: !Sub |
          var aws=require('aws-sdk');var response=require('cfn-response');exports.handler=function(event,context){console.log('Request received:\n',JSON.stringify(event));var physicalId=event.PhysicalResourceId;function success(data){data=data||{};console.log('SUCCESS:\n',data);return response.send(event,context,response.SUCCESS,data,physicalId)}
          function failed(err){console.log('FAILED:\n',err);return response.send(event,context,response.FAILED,err,physicalId)}
          if(event.RequestType!=='Delete'){console.log('Non-delete request is ignored');return success()}
          var snapshotId=event.ResourceProperties.SnapshotId;if(!snapshotId){return failed('SnapshotId required')}
          var ec2=new aws.EC2({region:event.ResourceProperties.Region});ec2.describeSnapshots({Filters:[{Name:'snapshot-id',Values:[snapshotId]}]}).promise().then((data)=>{console.log('"describeSnapshots" response:\n',JSON.stringify(data));if(!data.Snapshots.length){console.log('Snapshot not found');return null}
          return ec2.deleteSnapshot({SnapshotId:snapshotId}).promise()}).then((data)=>{console.log('"deleteSnapshot" response:\n',JSON.stringify(data));success()}).catch((err)=>failed(err))}
  DeleteSnapshotLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
              - lambda.amazonaws.com
          Action:
            - 'sts:AssumeRole'
      Path: /
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      - arn:aws:iam::aws:policy/service-role/AWSLambdaRole
      Policies:
      - PolicyName: EC2Policy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
            - Effect: Allow
              Action:
              - 'ec2:DescribeSnapshots'
              - 'ec2:DeleteSnapshot'
              Resource: ['*']
  DeleteSnapshotFunctionRetention:
    Type: Custom::LogsRetention
    DependsOn: []
    Properties:
      ServiceToken: !GetAtt SetLogsRetentionFunction.Arn
      LogGroupName: !Join ['', ['/aws/lambda/', !Ref DeleteSnapshotFunction]]
      RetentionInDays: 1

  # function to rename the snapshot that was used to create the volume
  RenameSnapshotFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt RenameSnapshotLambdaExecutionRole.Arn
      Runtime: nodejs4.3
      Timeout: 30
      Code:
        ZipFile: !Sub |
          var aws=require('aws-sdk');var response=require('cfn-response');exports.handler=function(event,context){console.log('Request received:\n',JSON.stringify(event));var physicalId=event.PhysicalResourceId;function success(data){data=data||{};console.log('SUCCESS:\n',data);return response.send(event,context,response.SUCCESS,data,physicalId)}
          function failed(err){console.log('FAILED:\n',err);return response.send(event,context,response.FAILED,err,physicalId)}
          if(event.RequestType!=='Delete'){console.log('Non-delete request is ignored');return success()}
          var snapshotId=event.ResourceProperties.SnapshotId;if(!snapshotId){return failed('SnapshotId required')}
          var ec2=new aws.EC2({region:event.ResourceProperties.Region});ec2.describeSnapshots({Filters:[{Name:'snapshot-id',Values:[snapshotId]}]}).promise().then((data)=>{console.log('"describeSnapshots" response:\n',JSON.stringify(data));if(!data.Snapshots.length){console.log('Snapshot not found');return null}
          var snapshotName=data.Snapshots[0].Tags.filter(function(el){return el.Key=='Name'})[0].Value;var snapshotDate=data.Snapshots[0].StartTime,newSnapshotName=snapshotName.substring(0,244)+'-'+Math.floor(snapshotDate.getTime()/1000);return ec2.createTags({Resources:[snapshotId],Tags:[{Key:'Name',Value:newSnapshotName}],}).promise()}).then((data)=>{console.log('"deleteSnapshot" response:\n',JSON.stringify(data));success()}).catch((err)=>failed(err))}
  RenameSnapshotLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
              - lambda.amazonaws.com
          Action:
            - 'sts:AssumeRole'
      Path: /
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      - arn:aws:iam::aws:policy/service-role/AWSLambdaRole
      Policies:
      - PolicyName: EC2Policy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
            - Effect: Allow
              Action:
              - 'ec2:DescribeSnapshots'
              - 'ec2:CreateTags'
              Resource: ['*']
  RenameSnapshotFunctionRetention:
    Type: Custom::LogsRetention
    DependsOn: []
    Properties:
      ServiceToken: !GetAtt SetLogsRetentionFunction.Arn
      LogGroupName: !Join ['', ['/aws/lambda/', !Ref RenameSnapshotFunction]]
      RetentionInDays: 1

  # function to set logs retention for a log group to 1 day (for lambdas by default they never expire)
  SetLogsRetentionFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt SetLogsRetentionLambdaExecutionRole.Arn
      Runtime: nodejs4.3
      Timeout: 30
      Code:
        ZipFile: |
          var aws=require("aws-sdk");var response=require('cfn-response');exports.handler=function(event,context){console.log("request received:\n"+JSON.stringify(event));var physicalId=event.PhysicalResourceId;function success(data){data=data||{}
          console.log('SUCCESS:\n',data);return response.send(event,context,response.SUCCESS,data,physicalId)}
          function failed(err){console.log('FAILED:\n',err);return response.send(event,context,response.FAILED,err,physicalId)}
          if(event.RequestType!=='Create'){console.log('Non-create request is ignored');return success()}
          var logGroupName=event.ResourceProperties.LogGroupName;if(!logGroupName){return failed('LogGroupName required')}
          var retentionInDays=event.ResourceProperties.RetentionInDays;if(!retentionInDays){return failed('RetentionInDays required')}
          var cloudwatchlogs=new aws.CloudWatchLogs();cloudwatchlogs.putRetentionPolicy({logGroupName:logGroupName,retentionInDays:retentionInDays}).promise().then((data)=>{console.log('"putRetentionPolicy" Response:\n',JSON.stringify(data));success()}).catch((err)=>failed(err))}
  SetLogsRetentionLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
              - lambda.amazonaws.com
          Action:
            - 'sts:AssumeRole'
      Path: /
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      - arn:aws:iam::aws:policy/service-role/AWSLambdaRole
      Policies:
      - PolicyName: CloudWatchLogsPolicy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
            - Effect: Allow
              Action:
              - logs:PutRetentionPolicy
              Resource:
              - arn:aws:logs:*:*:*
  SetLogsRetentionFunctionRetention:
    Type: Custom::LogsRetention
    Properties:
      ServiceToken: !GetAtt SetLogsRetentionFunction.Arn
      LogGroupName: !Join ['', ['/aws/lambda/', !Ref SetLogsRetentionFunction]]
      RetentionInDays: 1

  DockerReadyWaitCondition:
    Type: AWS::CloudFormation::WaitCondition
    DependsOn: SpotInstance
    CreationPolicy:
      ResourceSignal:
        Timeout: PT10M

Outputs:
  InstanceId:
    Value: !Ref SpotInstance
  InstanceIpAddress:
    Value: !GetAtt SpotInstance.PublicIp
  InstanceLogGroup:
    Value: !Ref SpotInstanceLogGroup
